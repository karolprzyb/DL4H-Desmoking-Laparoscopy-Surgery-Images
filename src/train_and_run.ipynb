{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "import json\n",
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import Adam\n",
    "from torch.optim import Optimizer\n",
    "                              \n",
    "from models import UNet\n",
    "from utils import *\n",
    "from losses import PSNR\n",
    "from SynthTrainer import SynthTrainer\n",
    "import arg_parser"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start by preparing our various models. First we will start with the model described in our target paper \"Desmoking Laparoscopy Surgery Images Using an Image-to-Image Translation Guided by an Embedded Dark Channel\"\n",
    "We will have to define a discriminator and a generator. Note the tables here reflect processing a 512x512 image as opposed to 256x256 as the original paper used. Also note that upon inspecting the paper repository, the tables in the paper appear inaccurate. I recreated these nets to be true to their repository by looking at the logic that generates the nets used.\n",
    "\n",
    "Since the arguments used were not specified in the paper I made a best estimate related to optional arguments based on what I thought would make sense. I also made some modification on where I apply dropout based on some advice from Dr. Florian Richter. It appears dropout was applied on all the 'decoder' layers in the unet, but it equally well could have not been applied at all depending on arguments passed at runtime. My assumption is dropout will be helpful and should be applied at the innermost layers of the decoder.\n",
    "\n",
    "ADD TABLES HERE AS IMAGES. KATEX DOES NOT IMPLEMENT TABULAR..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE PAPER DISCRIMINATOR\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, input_channels:int = 3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sequence = [torch.nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1), \n",
    "                    torch.nn.LeakyReLU(0.2, True)]\n",
    "        \n",
    "        sequence += [torch.nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), \n",
    "                     torch.nn.BatchNorm2d(128),\n",
    "                     torch.nn.LeakyReLU(0.2, True)]\n",
    "        \n",
    "        sequence += [torch.nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1), \n",
    "                     torch.nn.BatchNorm2d(256),\n",
    "                     torch.nn.LeakyReLU(0.2, True)]\n",
    "        \n",
    "        sequence += [torch.nn.ZeroPad2d(2)]\n",
    "        \n",
    "        sequence += [torch.nn.Conv2d(256, 512, kernel_size=4, stride=1), \n",
    "                     torch.nn.BatchNorm2d(512),\n",
    "                     torch.nn.LeakyReLU(0.2, True)]\n",
    "        \n",
    "        sequence += [torch.nn.ZeroPad2d(2)]\n",
    "        \n",
    "        sequence += [torch.nn.Conv2d(512, 1, kernel_size=4, stride=1)]\n",
    "       \n",
    "       # sequence += [torch.nn.Sigmoid()] #IT APPEARS THIS WAS NOT USED IN THE PAPER. COULD LOOK AT ITS IMPACT LATER POSSIBLY\n",
    "        \n",
    "        self.model = torch.nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the first unet which follows the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE PAPER UNET\n",
    "class UNET(torch.nn.Module):\n",
    "    def __init__(self, input_channels:int = 3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drp = torch.nn.dropout(0.5) # We will apply this in forward. Technically this could be in the list below instead too.\n",
    "\n",
    "        # Note that all convolutions with a BatchNorm after them have bias false as BatchNorm contains a bias term itself.\n",
    "        # It would therefore be redundant and add nothing.\n",
    "\n",
    "        sequence = [torch.nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1),                # 0         -> 42\n",
    "                    torch.nn.LeakyReLU(0.2, True)]                                                          # 1\n",
    "        \n",
    "        sequence += [torch.nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),              # 2\n",
    "                     torch.nn.BatchNorm2d(128),                                                             # 3         -> 39\n",
    "                     torch.nn.LeakyReLU(0.2, True)]                                                         # 4\n",
    "        sequence += [torch.nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),             # 5\n",
    "                     torch.nn.BatchNorm2d(256),                                                             # 6         -> 36\n",
    "                     torch.nn.LeakyReLU(0.2, True)]                                                         # 7\n",
    "        sequence += [torch.nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),             # 8\n",
    "                     torch.nn.BatchNorm2d(512),                                                             # 9         -> 33\n",
    "                     torch.nn.LeakyReLU(0.2, True)]                                                         # 10\n",
    "        \n",
    "        sequence += [torch.nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1, bias=False),             # 11\n",
    "                     torch.nn.BatchNorm2d(512),                                                             # 12        -> 30\n",
    "                     torch.nn.LeakyReLU(0.2, True)]                                                         # 13\n",
    "        sequence += [torch.nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1, bias=False),             # 14\n",
    "                     torch.nn.BatchNorm2d(512),                                                             # 15        -> 27\n",
    "                     torch.nn.LeakyReLU(0.2, True)]                                                         # 16\n",
    "        sequence += [torch.nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1, bias=False),             # 17\n",
    "                     torch.nn.BatchNorm2d(512),                                                             # 18        -> 24\n",
    "                     torch.nn.LeakyReLU(0.2, True)]                                                         # 19\n",
    "        \n",
    "        sequence += [torch.nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1),                         # 20\n",
    "                     torch.nn.ReLU(True)]                                                                   # 21\n",
    "        sequence += [torch.nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2, padding=1, bias=False),    # 22\n",
    "                     torch.nn.BatchNorm2d(512),                                                             # 23\n",
    "                     torch.nn.ReLU(True)]                                                                   # 24        <- 18\n",
    "        \n",
    "        sequence += [torch.nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1, bias=False),   # 25\n",
    "                     torch.nn.BatchNorm2d(512),                                                             # 26\n",
    "                     torch.nn.ReLU(True)]                                                                   # 27        <- 15\n",
    "        sequence += [torch.nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1, bias=False),   # 28\n",
    "                     torch.nn.BatchNorm2d(512),                                                             # 29\n",
    "                     torch.nn.ReLU(True)]                                                                   # 30        <- 12\n",
    "        sequence += [torch.nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1, bias=False),   # 31\n",
    "                     torch.nn.BatchNorm2d(512),                                                             # 32\n",
    "                     torch.nn.ReLU(True)]                                                                   # 33        <- 9\n",
    "        \n",
    "        sequence += [torch.nn.ConvTranspose2d(1024, 256, kernel_size=4, stride=2, padding=1, bias=False),   # 34\n",
    "                     torch.nn.BatchNorm2d(256),                                                             # 35\n",
    "                     torch.nn.ReLU(True)]                                                                   # 36        <- 6\n",
    "        sequence += [torch.nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1, bias=False),    # 37\n",
    "                     torch.nn.BatchNorm2d(128),                                                             # 38\n",
    "                     torch.nn.ReLU(True)]                                                                   # 39        <- 3\n",
    "        sequence += [torch.nn.ConvTranspose2d(1024, 64, kernel_size=4, stride=2, padding=1, bias=False),    # 40\n",
    "                     torch.nn.BatchNorm2d(64),                                                              # 41\n",
    "                     torch.nn.ReLU(True)]                                                                   # 42        <- 0\n",
    "        \n",
    "        sequence += [torch.nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1),                  # 43\n",
    "                     torch.nn.Tanh()]                                                                       # 44\n",
    "        \n",
    "        for x in range(0, 20):\n",
    "            if type(sequence[x]) == torch.nn.modules.conv.Conv2d:\n",
    "                torch.nn.init.kaiming_uniform_(sequence[x].weight, a=0.2, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        \n",
    "        for x in range(20, len(sequence)):\n",
    "            if type(sequence[x]) == torch.nn.modules.conv.Conv2d or type(sequence[x]) == torch.nn.modules.conv.conTranspose2d:\n",
    "                torch.nn.init.kaiming_uniform_(sequence[x].weight, mode='fan_in', nonlinearity='relu')\n",
    "        \n",
    "        for x in range(0, len(sequence)):\n",
    "            if type(sequence[x]) == torch.nn.modules.batchnorm.BatchNorm2d:\n",
    "                torch.nn.init.constant_(sequence[x].bias.data, 0.0)\n",
    "                torch.nn.init.normal_(sequence[x].weight.data, 1.0, 0.02)\n",
    "\n",
    "        self.sequence = sequence\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        x0 = x = self.sequence[0](x)\n",
    "        self.sequence[1](x)\n",
    "        x = self.sequence[2](x)\n",
    "        x3 = x = self.sequence[3](x)\n",
    "        self.sequence[4](x)\n",
    "        x = self.sequence[5](x)\n",
    "        x6 = x = self.sequence[6](x)\n",
    "        self.sequence[7](x)\n",
    "        x = self.sequence[8](x)\n",
    "        x9 = x = self.sequence[9](x)\n",
    "        self.sequence[10](x)\n",
    "        x = self.sequence[11](x)\n",
    "        x12 = x = self.sequence[12](x)\n",
    "        self.sequence[13](x)\n",
    "        x = self.sequence[14](x)\n",
    "        x15 = x = self.sequence[15](x)\n",
    "        self.sequence[16](x)\n",
    "        x = self.sequence[17](x)\n",
    "        x18 = x = self.sequence[18](x)\n",
    "        self.sequence[19](x)\n",
    "        x = self.sequence[20](x)\n",
    "        self.sequence[21](x)\n",
    "        x = self.sequence[22](x)\n",
    "        x = self.sequence[23](x)\n",
    "        x = torch.cat(x, x18, 1)\n",
    "        self.sequence[24](x)\n",
    "        x = self.sequence[25](x)\n",
    "        x = self.sequence[26](x)\n",
    "        x = torch.cat(x, x15, 1)\n",
    "        x = self.sequence[27](x)\n",
    "        x = self.sequence[28](x)\n",
    "        x = self.sequence[29](x)\n",
    "        x = torch.cat(x, x12, 1)\n",
    "        x = self.sequence[30](x)\n",
    "        x = self.sequence[31](x)\n",
    "        x = self.sequence[32](x)\n",
    "        x = torch.cat(x, x9, 1)\n",
    "        x = self.sequence[33](x)\n",
    "        x = self.sequence[34](x)\n",
    "        x = self.sequence[35](x)\n",
    "        x = torch.cat(x, x6, 1)\n",
    "        x = self.sequence[36](x)\n",
    "        x = self.sequence[37](x)\n",
    "        x = self.sequence[38](x)\n",
    "        x = torch.cat(x, x3, 1)\n",
    "        x = self.sequence[39](x)\n",
    "        x = self.sequence[40](x)\n",
    "        x = self.sequence[41](x)\n",
    "        x = torch.cat(x, x0, 1)\n",
    "        x = self.sequence[42](x)\n",
    "        x = self.sequence[43](x)\n",
    "        return self.sequence[44](x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our unet with ablations. Layers xxxx are taken out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE ABLATED UNET\n",
    "class UNETsmolr(torch.nn.Module):\n",
    "    def __init__(self, input_channels:int = 3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drp = torch.nn.dropout(0.5) # We will apply this in forward. Technically this could be in the list below instead too.\n",
    "\n",
    "        # Note that instead of changing the number of entries, I substituted the parts of the net I planned to ablate with None\n",
    "        # and removed them from forward. This means I don't have to bother as much with changing indices.\n",
    "\n",
    "        # Note that all convolutions with a BatchNorm after them have bias false as BatchNorm contains a bias term itself.\n",
    "        # It would therefore be redundant and add nothing.\n",
    "\n",
    "        sequence = [torch.nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1),                # 0         -> 42\n",
    "                    torch.nn.LeakyReLU(0.2, True)]                                                          # 1\n",
    "        \n",
    "        sequence += [torch.nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),              # 2\n",
    "                     torch.nn.BatchNorm2d(128),                                                             # 3         -> 39\n",
    "                     torch.nn.LeakyReLU(0.2, True)]                                                         # 4\n",
    "        sequence += [torch.nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),             # 5\n",
    "                     torch.nn.BatchNorm2d(256),                                                             # 6         -> 36\n",
    "                     torch.nn.LeakyReLU(0.2, True)]                                                         # 7\n",
    "        sequence += [torch.nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),             # 8\n",
    "                     torch.nn.BatchNorm2d(512),                                                             # 9         -> 33\n",
    "                     torch.nn.LeakyReLU(0.2, True)]                                                         # 10\n",
    "        \n",
    "        sequence += [torch.nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1, bias=False),             # 11\n",
    "                     torch.nn.BatchNorm2d(512),                                                             # 12        -> 30\n",
    "                     torch.nn.LeakyReLU(0.2, True)]                                                         # 13\n",
    "        sequence += [torch.nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1, bias=False),             # 14\n",
    "                     torch.nn.BatchNorm2d(512),                                                             # 15        -> 27\n",
    "                     torch.nn.LeakyReLU(0.2, True)]                                                         # 16\n",
    "        sequence += [torch.nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1, bias=False),             # 17\n",
    "                     torch.nn.BatchNorm2d(512),                                                             # 18        -> 24\n",
    "                     torch.nn.LeakyReLU(0.2, True)]                                                         # 19\n",
    "        \n",
    "        sequence += [torch.nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1),                         # 20\n",
    "                     torch.nn.ReLU(True)]                                                                   # 21\n",
    "        sequence += [torch.nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2, padding=1, bias=False),    # 22\n",
    "                     torch.nn.BatchNorm2d(512),                                                             # 23\n",
    "                     torch.nn.ReLU(True)]                                                                   # 24        <- 18\n",
    "        \n",
    "        sequence += [torch.nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1, bias=False),   # 25\n",
    "                     torch.nn.BatchNorm2d(512),                                                             # 26\n",
    "                     torch.nn.ReLU(True)]                                                                   # 27        <- 15\n",
    "        sequence += [torch.nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1, bias=False),   # 28\n",
    "                     torch.nn.BatchNorm2d(512),                                                             # 29\n",
    "                     torch.nn.ReLU(True)]                                                                   # 30        <- 12\n",
    "        sequence += [torch.nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1, bias=False),   # 31\n",
    "                     torch.nn.BatchNorm2d(512),                                                             # 32\n",
    "                     torch.nn.ReLU(True)]                                                                   # 33        <- 9\n",
    "        \n",
    "        sequence += [torch.nn.ConvTranspose2d(1024, 256, kernel_size=4, stride=2, padding=1, bias=False),   # 34\n",
    "                     torch.nn.BatchNorm2d(256),                                                             # 35\n",
    "                     torch.nn.ReLU(True)]                                                                   # 36        <- 6\n",
    "        sequence += [torch.nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1, bias=False),    # 37\n",
    "                     torch.nn.BatchNorm2d(128),                                                             # 38\n",
    "                     torch.nn.ReLU(True)]                                                                   # 39        <- 3\n",
    "        sequence += [torch.nn.ConvTranspose2d(1024, 64, kernel_size=4, stride=2, padding=1, bias=False),    # 40\n",
    "                     torch.nn.BatchNorm2d(64),                                                              # 41\n",
    "                     torch.nn.ReLU(True)]                                                                   # 42        <- 0\n",
    "        \n",
    "        sequence += [torch.nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1),                  # 43\n",
    "                     torch.nn.Tanh()]                                                                       # 44\n",
    "        \n",
    "        # Initilizing here. Initializing the generator should have impact, so making sure to do that.\n",
    "\n",
    "        for x in range(0, 20):\n",
    "            if type(sequence[x]) == torch.nn.modules.conv.Conv2d:\n",
    "                torch.nn.init.kaiming_uniform_(sequence[x].weight, a=0.2, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        \n",
    "        for x in range(20, len(sequence)):\n",
    "            if type(sequence[x]) == torch.nn.modules.conv.Conv2d or type(sequence[x]) == torch.nn.modules.conv.conTranspose2d:\n",
    "                torch.nn.init.kaiming_uniform_(sequence[x].weight, mode='fan_in', nonlinearity='relu')\n",
    "        \n",
    "        for x in range(0, len(sequence)):\n",
    "            if type(sequence[x]) == torch.nn.modules.batchnorm.BatchNorm2d:\n",
    "                torch.nn.init.constant_(sequence[x].bias.data, 0.0)\n",
    "                torch.nn.init.normal_(sequence[x].weight.data, 1.0, 0.02)\n",
    "\n",
    "        self.sequence = sequence\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        x0 = x = self.sequence[0](x)\n",
    "        self.sequence[1](x)\n",
    "        x = self.sequence[2](x)\n",
    "        x3 = x = self.sequence[3](x)\n",
    "        self.sequence[4](x)\n",
    "        x = self.sequence[5](x)\n",
    "        x6 = x = self.sequence[6](x)\n",
    "        self.sequence[7](x)\n",
    "        x = self.sequence[8](x)\n",
    "        x9 = x = self.sequence[9](x)\n",
    "        self.sequence[10](x)\n",
    "        x = self.sequence[11](x)\n",
    "        x12 = x = self.sequence[12](x)\n",
    "        self.sequence[13](x)\n",
    "        x = self.sequence[14](x)\n",
    "        x15 = x = self.sequence[15](x)\n",
    "        self.sequence[16](x)\n",
    "        x = self.sequence[17](x)\n",
    "        x18 = x = self.sequence[18](x)\n",
    "        self.sequence[19](x)\n",
    "        x = self.sequence[20](x)\n",
    "        self.sequence[21](x)\n",
    "        x = self.sequence[22](x)\n",
    "        x = self.sequence[23](x)\n",
    "        x = torch.cat(x, x18, 1)\n",
    "        self.sequence[24](x)\n",
    "        x = self.sequence[25](x)\n",
    "        x = self.sequence[26](x)\n",
    "        x = torch.cat(x, x15, 1)\n",
    "        x = self.sequence[27](x)\n",
    "        x = self.sequence[28](x)\n",
    "        x = self.sequence[29](x)\n",
    "        x = torch.cat(x, x12, 1)\n",
    "        x = self.sequence[30](x)\n",
    "        x = self.sequence[31](x)\n",
    "        x = self.sequence[32](x)\n",
    "        x = torch.cat(x, x9, 1)\n",
    "        x = self.sequence[33](x)\n",
    "        x = self.sequence[34](x)\n",
    "        x = self.sequence[35](x)\n",
    "        x = torch.cat(x, x6, 1)\n",
    "        x = self.sequence[36](x)\n",
    "        x = self.sequence[37](x)\n",
    "        x = self.sequence[38](x)\n",
    "        x = torch.cat(x, x3, 1)\n",
    "        x = self.sequence[39](x)\n",
    "        x = self.sequence[40](x)\n",
    "        x = self.sequence[41](x)\n",
    "        x = torch.cat(x, x0, 1)\n",
    "        x = self.sequence[42](x)\n",
    "        x = self.sequence[43](x)\n",
    "        return self.sequence[44](x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to write our training code. There will be differences here for loading each variation we intend to run (4 in all). Regular and ablated with and without dark channel. In all cases the discriminator remains the same other than input channels which are set by the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WRITE TRAINING CODE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to write code to run our models on a test dataset. Technically this is not necessary as we are not doing parameter tuning. But this does give us an idea of how the model performs on a video that it didn't even have a portion of fed in for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WRITE TESTING CODE FOR TRAINED MODEL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training and validation for scenario 1. This is full u-net and no dark channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE TO TRAIN SCENARIO 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training and validation for scenario 2. This is ablated u-net and no dark channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE TO TRAIN SCENARIO 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training and validation for scenario 3. This is full u-net and dark channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE TO TRAIN SCENARIO 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training and validation for scenario 4. This is ablated u-net and dark channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE TO TRAIN SCENARIO 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must run testing on all 4 scenarios. Load in all the models at each epoch so we can plot metrics. (INTEGRATE THIS INTO TRAIN AND VALIDATE??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YES?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL4H_CS598",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
