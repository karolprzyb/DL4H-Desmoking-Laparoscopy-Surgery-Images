{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "import json\n",
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import Adam\n",
    "from torch.optim import Optimizer\n",
    "                              \n",
    "from models import UNet\n",
    "from utils import *\n",
    "from losses import PSNR\n",
    "from SynthTrainer import SynthTrainer\n",
    "import arg_parser"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start by preparing our various models. First we will start with the model described in our target paper \"Desmoking Laparoscopy Surgery Images Using an Image-to-Image Translation Guided by an Embedded Dark Channel\"\n",
    "We will have to define a discriminator and a generator. Note the tables here reflect processing a 512x512 image as opposed to 256x256 as the original paper used.\n",
    "\n",
    "ADD TABLES HERE AS IMAGES. KATEX DOES NOT IMPLEMENT TABULAR..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE PAPER DISCRIMINATOR\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, input_channels:int = 3):\n",
    "        super().__init__()\n",
    "\n",
    "        sequence = [torch.nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1), \n",
    "                    torch.nn.LeakyReLU(0.2, True)]\n",
    "        \n",
    "        sequence += [torch.nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), \n",
    "                     torch.nn.BatchNorm2d(128),\n",
    "                     torch.nn.LeakyReLU(0.2, True)]\n",
    "        \n",
    "        sequence += [torch.nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1), \n",
    "                     torch.nn.BatchNorm2d(256),\n",
    "                     torch.nn.LeakyReLU(0.2, True)]\n",
    "        \n",
    "        sequence += [torch.nn.ZeroPad2d(2)]\n",
    "        \n",
    "        sequence += [torch.nn.Conv2d(256, 512, kernel_size=4, stride=1), \n",
    "                     torch.nn.BatchNorm2d(512),\n",
    "                     torch.nn.LeakyReLU(0.2, True)]\n",
    "        \n",
    "        sequence += [torch.nn.ZeroPad2d(2)]\n",
    "        \n",
    "        sequence += [torch.nn.Conv2d(512, 1, kernel_size=4, stride=1)]\n",
    "       \n",
    "       # sequence += [torch.nn.Sigmoid()] #IT APPEARS THIS WAS NOT USED IN THE PAPER. COULD LOOK AT ITS IMPACT LATER POSSIBLY\n",
    "        \n",
    "        self.model = torch.nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the first unet which follows the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE PAPER UNET\n",
    "class UNETplus(torch.nn.Module):\n",
    "    def __init__(self, input_channels:int = 3):\n",
    "        super().__init__()\n",
    "\n",
    "        sequence = [torch.nn.Conv2d()]\n",
    "\n",
    "\n",
    "\n",
    "        sequence = [torch.nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1), \n",
    "                    torch.nn.LeakyReLU(0.2, True)]\n",
    "        \n",
    "        sequence += [torch.nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), \n",
    "                     torch.nn.BatchNorm2d(128),\n",
    "                     torch.nn.LeakyReLU(0.2, True)]\n",
    "        \n",
    "        sequence += [torch.nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1), \n",
    "                     torch.nn.BatchNorm2d(256),\n",
    "                     torch.nn.LeakyReLU(0.2, True)]\n",
    "        \n",
    "        sequence += [torch.nn.ZeroPad2d(2)]\n",
    "        \n",
    "        sequence += [torch.nn.Conv2d(256, 512, kernel_size=4, stride=1), \n",
    "                     torch.nn.BatchNorm2d(512),\n",
    "                     torch.nn.LeakyReLU(0.2, True)]\n",
    "        \n",
    "        sequence += [torch.nn.ZeroPad2d(2)]\n",
    "        \n",
    "        sequence += [torch.nn.Conv2d(512, 1, kernel_size=4, stride=1)]\n",
    "        \n",
    "        self.model = torch.nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our unet with ablations. Layers xxxx are taken out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE ABLATED UNET"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to write our training code. There will be differences here for loading each variation we intend to run (4 in all). Regular and ablated with and without dark channel. In all cases the discriminator remains the same other than input channels which are set by the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WRITE TRAINING CODE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to write code to run our models on a test dataset. Technically this is not necessary as we are not doing parameter tuning. But this does give us an idea of how the model performs on a video that it didn't even have a portion of fed in for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WRITE TESTING CODE FOR TRAINED MODEL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training and validation for scenario 1. This is full u-net and no dark channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE TO TRAIN SCENARIO 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training and validation for scenario 2. This is ablated u-net and no dark channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE TO TRAIN SCENARIO 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training and validation for scenario 3. This is full u-net and dark channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE TO TRAIN SCENARIO 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training and validation for scenario 4. This is ablated u-net and dark channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE TO TRAIN SCENARIO 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must run testing on all 4 scenarios. Load in all the models at each epoch so we can plot metrics. (INTEGRATE THIS INTO TRAIN AND VALIDATE??)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL4H_CS598",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
